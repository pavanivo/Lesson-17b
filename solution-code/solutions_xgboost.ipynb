{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right now I am getting a useless warning every time I fit an `XGBoost` model.\n",
    "# This line of code prevents warnings from being displayed. Not generally\n",
    "# recommended.\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Gradient boosting,\" like bagging, is a general method for training decision tree ensembles.\n",
    "\n",
    "XGBoost (\"eXtreme Gradient Boosting\") is a particular implementation of gradient boosted decision trees. It is popular on Kaggle because it is both fast to train and often gives excellent predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `xgboost` library instead of scikit-learn for this lesson. Scikit-learn has [`GradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) and [`GradientBoostingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) classes, but they lack some of the tricks that have made the `xgboost` library popular.\n",
    "\n",
    "`xgboost` provides estimators that use the same interface as scikit-learn's, so we will not need to change our approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the xgboost package\n",
    "# /scrub/\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an XGBoost regressor\n",
    "# /scrub/\n",
    "xgb_reg = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise (20 mins., in pairs)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the Ames housing dataset from `assets/data/ames_train.csv` in this lesson's base directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /scrub/\n",
    "ames_df = pd.read_csv('../assets/data/ames_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a feature matrix DataFrame `X` containing all of the numeric columns from the Ames dataset except \"Id\" and the target column \"SalePrice\". Drop \"OverallQual\" to make things more interesting -- that very is very predictive but expensive to collect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /scrub/\n",
    "target_col = 'SalePrice'\n",
    "\n",
    "X = (ames_df\n",
    "     .select_dtypes(['int64', 'float64'])\n",
    "     .drop(['Id', 'OverallQual', target_col], axis='columns')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a target vector Series `y` with the values of the variable \"SalePrice\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /scrub/\n",
    "y = ames_df.loc[:, target_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do a simple train/test split on `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /scrub/\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fit an `XGBRegressor` on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "       importance_type='gain', interaction_constraints='',\n",
       "       learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "       min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "       n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "       objective='reg:squarederror', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "       validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# /scrub/\n",
    "xgb_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get an R^2 score for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999806045456122"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# /scrub/\n",
    "xgb_reg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get an R^2 score for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8379362134973047"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# /scrub/\n",
    "xgb_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Is your model overfitting, underfitting, both, or neither? How do you know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "Overfitting somewhat. The score on the training set is close to 1, so it does not seem to be underfitting, and the gap between training and test scores is moderately large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the Titanic dataset (located in this lesson's  `assets/data` directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /scrub/\n",
    "titanic = pd.read_csv('../assets/data/titanic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a feature matrix `X` by dropping \"PassengerId\", \"Name\", \"Ticket\", and the target column \"Survived.\" Dummy-code string columns as needed. Do NOT drop or impute missing values -- XGBoost handles them internally!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media.giphy.com/media/12NUbkX6p4xOO4/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /scrub/\n",
    "X = titanic.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'Survived'], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /scrub/\n",
    "X = pd.get_dummies(X, columns=['Sex', 'Embarked'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a target Series `y` with the values of \"Survived.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /scrub/\n",
    "y = titanic.loc[:, 'Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instantiate an `XGBClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /scrub/\n",
    "xgb_clas = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fit and score the classifier on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9708193041526375"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# /scrub/\n",
    "xgb_clas.fit(X, y)\n",
    "xgb_clas.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get your classifier's score on held-out data using ten-fold cross-validation on the Titanic dataset, shuffling the rows before taking the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /scrub/\n",
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /scrub/\n",
    "kf = KFold(10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /scrub/\n",
    "scores = cross_val_score(xgb_clas, X, y, cv=kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8204369538077403"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# /scrub/\n",
    "np.array(scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Is your model overfitting, underfitting, both, or neither? How do you know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "Maybe neither, or slightly overfitting. The score on the training set seems pretty good, so it does not seem to be underfitting, and the gap between training and test scores is not very big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## XGBoost vs. Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests and XGBoost both produce tree ensembles, and the provide many of the same parameters to reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting vs. Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bagging involves training each tree *independently* on a different *bootstrap sample*.\n",
    "- Gradient boosting involves training each tree *sequentially to reduce the residual errors left by its predecessors*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [the official `xgboost` library documentation](https://xgboost.readthedocs.io/en/latest/tutorials/model.html) and Chapter 10 of [Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each split for a given variable, XGBoost simply learns whether sending items with missing values left or right gives better results. This approach has a few advantages:\n",
    "\n",
    "- It is automatic.\n",
    "- Unlike dropping rows or columns, it allows you to use all of the values you do have.\n",
    "- Unlike imputation, it treats \"missing\" as its own value rather than replacing it with some other value that might be wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Tuning XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`XGBoost` provides many options that you can tune to improve predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `n_estimators` and `learning_rate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `learning_rate` controls how \"aggressive\" each tree is in trying to correct the errors of its predecessors.\n",
    "\n",
    "- If it is too low, then getting good predictive performance will require a large value for `n_estimators` (and thus a lot of time).\n",
    "- If it is too high, then the algorithm will keep overshooting the target and won't coverge to good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike with a random forest, setting `n_estimators` too high can hurt predictive performance with boosting because it leads to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Addressing Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing Model Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to address overfitting is to restrict model complexity more or less directly. `xgboost` provides many options for this purpose:\n",
    "\n",
    "- Restricting tree shape\n",
    "    - `max_depth` / `max_leaf_nodes` puts a hard limit on the depth or number of leaves in each tree\n",
    "    - `gamma` is the minimum loss reduction required in order to make another split\n",
    "    - `min_child_weight` is the minimum number of observations required in each child node in order to make a split, adjusted for the weight that is placed on each observation\n",
    "- Restricting sizes of weights: `reg_lambda` and `reg_alpha` provide L1 and L2 regularization on sample weights, respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to address overfitting when ensembling is to add randomness to the process of training each item in the ensemble.\n",
    "\n",
    "- `subsample` specifies what proportion of the data is used to train each tree.\n",
    "- `colsample_bytree` and `colsample_bylevel` specify what proportion of the features are available at the tree and split level, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this general approach to tune our model:\n",
    "\n",
    "- Find the optimal number of trees with default learning rate.\n",
    "- Tune additional parameters.\n",
    "- Lower learning rate and increase the number of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Optimal Number of Trees with Default Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data by column\n",
    "# /scrub/\n",
    "target_col = 'SalePrice'\n",
    "\n",
    "X = ames_df.select_dtypes(['int64', 'float64']).drop(['Id', 'OverallQual', target_col], axis='columns')\n",
    "y = ames_df.loc[:, target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "# /scrub/\n",
    "xgb_reg = xgb.XGBRegressor(learning_rate=0.1, max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8529772770319743\n"
     ]
    }
   ],
   "source": [
    "# Fit and score on all data\n",
    "# /scrub/\n",
    "xgb_reg.fit(X, y)\n",
    "print(xgb_reg.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.805096625229195"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score with 5-fold CV\n",
    "# /scrub/\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "np.array(cross_val_score(xgb_reg, X, y, cv=kf)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.8366004966425535\n",
      "200 0.8375681550240405\n",
      "300 0.8359383418375476\n",
      "400 0.8340299165071441\n",
      "500 0.8322833691133817\n",
      "600 0.831478123416369\n",
      "700 0.8310230844903334\n",
      "800 0.8302473936730767\n",
      "900 0.8290506194525127\n"
     ]
    }
   ],
   "source": [
    "# Vary number of trees\n",
    "# /scrub/\n",
    "for num_trees in range(100, 1000, 100):\n",
    "    xgb_reg = xgb.XGBRegressor(n_estimators=num_trees, max_depth=1)\n",
    "    print(num_trees, np.array(cross_val_score(xgb_reg, X, y, cv=kf)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix number of trees\n",
    "# /scrub/\n",
    "xgb_reg = xgb.XGBRegressor(n_estimators=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune Additional Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn has a `GridSearchCV` class that will run a model with various hyperparameter combinations and identify the combination that generated the best cross-validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=XGBRegressor(base_score=None, booster=None, colsample_bylevel=None,\n",
       "       colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
       "       gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
       "       learning_rate=None, max_delta_step=None, max_depth=None,\n",
       "       min_child_we..._pos_weight=None, subsample=None,\n",
       "       tree_method=None, validate_parameters=None, verbosity=None),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'max_depth': [1, 4, 7], 'min_child_weight': [1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try a few values for \"max_depth\" and \"min_child_weight\"\n",
    "# /scrub/\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [1, 4, 7], 'min_child_weight': [1, 10, 100]\n",
    "}\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_reg,\n",
    "    param_grid=param_grid,\n",
    "    cv=kf\n",
    ")\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_depth': 1, 'min_child_weight': 1}, 0.8501283922179043)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find out best parameters and their score\n",
    "# /scrub/\n",
    "grid_search.best_params_, grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise-deprecating',\n",
       "       estimator=XGBRegressor(base_score=None, booster=None, colsample_bylevel=None,\n",
       "       colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
       "       gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
       "       learning_rate=None, max_delta_step=None, max_depth=4,\n",
       "       min_child_weigh..._pos_weight=None, subsample=None,\n",
       "       tree_method=None, validate_parameters=None, verbosity=None),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'subsample': array([0.5    , 0.66667, 0.83333, 1.     ])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try a few values for \"subsample\"\n",
    "# /scrub/\n",
    "param_grid = {\n",
    "    'subsample': np.linspace(0.5, 1, 4)\n",
    "}\n",
    "xgb_reg = xgb.XGBRegressor(n_estimators=500, max_depth=4, min_child_weight=1)\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_reg, \n",
    "    param_grid=param_grid,\n",
    "    cv=4\n",
    ")\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'subsample': 1.0}, 0.8302317921178423)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find out best parameters and their score\n",
    "# /scrub/\n",
    "grid_search.best_params_, grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.630490</td>\n",
       "      <td>0.012615</td>\n",
       "      <td>0.007188</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'subsample': 0.5}</td>\n",
       "      <td>0.747250</td>\n",
       "      <td>0.829442</td>\n",
       "      <td>0.841482</td>\n",
       "      <td>0.759477</td>\n",
       "      <td>0.794444</td>\n",
       "      <td>0.041501</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.641308</td>\n",
       "      <td>0.006942</td>\n",
       "      <td>0.007460</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>{'subsample': 0.6666666666666666}</td>\n",
       "      <td>0.745475</td>\n",
       "      <td>0.879521</td>\n",
       "      <td>0.822575</td>\n",
       "      <td>0.841460</td>\n",
       "      <td>0.822240</td>\n",
       "      <td>0.048864</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.685030</td>\n",
       "      <td>0.031581</td>\n",
       "      <td>0.007378</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>{'subsample': 0.8333333333333333}</td>\n",
       "      <td>0.774624</td>\n",
       "      <td>0.810427</td>\n",
       "      <td>0.837766</td>\n",
       "      <td>0.799023</td>\n",
       "      <td>0.805466</td>\n",
       "      <td>0.022707</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.662147</td>\n",
       "      <td>0.008810</td>\n",
       "      <td>0.006888</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>1</td>\n",
       "      <td>{'subsample': 1.0}</td>\n",
       "      <td>0.794896</td>\n",
       "      <td>0.864909</td>\n",
       "      <td>0.844113</td>\n",
       "      <td>0.816961</td>\n",
       "      <td>0.830232</td>\n",
       "      <td>0.026561</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.630490      0.012615         0.007188        0.000998   \n",
       "1       0.641308      0.006942         0.007460        0.000398   \n",
       "2       0.685030      0.031581         0.007378        0.000605   \n",
       "3       0.662147      0.008810         0.006888        0.000425   \n",
       "\n",
       "  param_subsample                             params  split0_test_score  \\\n",
       "0             0.5                 {'subsample': 0.5}           0.747250   \n",
       "1        0.666667  {'subsample': 0.6666666666666666}           0.745475   \n",
       "2        0.833333  {'subsample': 0.8333333333333333}           0.774624   \n",
       "3               1                 {'subsample': 1.0}           0.794896   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
       "0           0.829442           0.841482           0.759477         0.794444   \n",
       "1           0.879521           0.822575           0.841460         0.822240   \n",
       "2           0.810427           0.837766           0.799023         0.805466   \n",
       "3           0.864909           0.844113           0.816961         0.830232   \n",
       "\n",
       "   std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0        0.041501                4            0.999998            0.999997   \n",
       "1        0.048864                2            0.999999            1.000000   \n",
       "2        0.022707                3            1.000000            1.000000   \n",
       "3        0.026561                1            0.999999            0.999999   \n",
       "\n",
       "   split2_train_score  split3_train_score  mean_train_score  std_train_score  \n",
       "0            0.999987            0.999989          0.999993         0.000005  \n",
       "1            0.999991            0.999990          0.999995         0.000004  \n",
       "2            0.999992            0.999992          0.999996         0.000004  \n",
       "3            0.999993            0.999993          0.999996         0.000003  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get report on grid search results\n",
    "# /scrub/\n",
    "pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect of one hyperparameter typically depends on the values of other hyperparameters -- for instance, increasing \"max_depth\" will have no effect if \"min_child_weight\" is sufficiently large. For this reason, it is generally valuable to do grid searches over multiple parameters simultaneously, rather than fixing one hyperparameter at a time. However, testing many combinations of many parameters can take a long time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lower Learning Rate and Increase the Number of Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 -0.08719699741574949\n",
      "600 0.8563178936418027\n",
      "1100 0.8628480853282022\n",
      "1600 0.864961902310833\n",
      "2100 0.8656006970610834\n",
      "2600 0.8662852078324466\n",
      "3100 0.866567178314925\n"
     ]
    }
   ],
   "source": [
    "# Divide the learning rate by 10 and vary number of trees\n",
    "# /scrub/\n",
    "for num_trees in range(100, 3101, 500):\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "        learning_rate=.01,\n",
    "        n_estimators=num_trees,\n",
    "        max_depth=4,\n",
    "        min_child_weight=1,\n",
    "        subsample=0.67,\n",
    "    )\n",
    "    print(num_trees, np.array(cross_val_score(xgb_reg, X, y, cv=kf)).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise (open-ended, in pairs)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the best XGBoost model you can for the Titanic dataset, as measured by accuracy in five-fold cross-validation. Use `GridSearchCV` at least once to search over at least two hyperparameters at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `XGBoost` is a popular decision tree ensemble algorithm.\n",
    "- `XGBoost` uses gradient boosting, meaning that each tree attempts to correct the errors of previous trees.\n",
    "- scikit-learn's `GridSearchCV` helps with testing hyperparameter values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
